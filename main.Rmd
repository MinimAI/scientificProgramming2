---
title: "Making multivariate statistics reproducible"
output: html_notebook
thanks: "Replication files are available on the author's Github account."
author:
- name: Felix Taschbach
  affiliation: Maastricht University
---
## Load Packages
We begin by installing and loading any required packages. The following code first installs the package pacman, if necessary, and then uses pacman to install, if required, and load the packages needed to run this script.
```{r}
# Clear all variables 
rm(list=ls()) 

# install pacman if necessary
if (!require("pacman")) suppressPackageStartupMessages(install.packages("pacman"))
# install required packages
pacman::p_load("ggplot2", "dplyr","pls","readr","caTools","caret","gplots","car","rcdk","iterators","lattice","latticeExtra")
```
## Importing the data.
To import the data I am first setting the local path to the directory that the data is stored in.
```{r, setup}
DATA.DIR <- "/Users/felix/Google Drive/Uni/Systems Biology/Year 2/Scientific Programming/scientificProgramming2/"
knitr::opts_chunk$set(root.dir = DATA.DIR) # set the working directory in the current chunk
```
Afterwards, I import the activity scores. To properly import the activity scores matrix, 2 import function calls are needed: one to import the header and one to import the data.
```{r}
# import the header and the activity scores
headers   <- read.csv('aid624202.csv', header=FALSE, nrows=1, as.is=TRUE, row=1)
data.raw  <- read.csv('aid624202.csv',skip=6,header=FALSE,row=1)
colnames(data.raw) <- headers                                       # change the names of the data to the correct ones
data.raw  <- data.raw[,c("PUBCHEM_SID","PUBCHEM_ACTIVITY_SCORE")]   # select SID and the activity score from the data
colnames(data.raw) <- c("SID","Score")# change column names
```
### Computing the Descriptor matrix from the `.sdf` file
The following code computes the descriptor matrix from a `.sdf` file. It takes a long time to run (and installing the packages like rJava, required for it to run, is also annoying). That is why the output is saved and the evaluation is set to false. Thus, you can run it once to create the descriptor matrix, and then, use the saved version, instead of recomputing it, on further runs.
```{r eval = FALSE}
inactives <- which(data.raw[,"Score"] < 10)   # select all inactive molecules
actives   <- which(data.raw[,"Score"] >= 10)  # select all active molecules (activity score >= 10)
cat("Amount of inactive molecules: ", length(inactives))

num_inactives <- 300
num_actives   <- 750
num_total     <- num_inactives + num_actives

selectedInactives <- sample(inactives, num_inactives)
selectedActives   <- sample(actives, num_actives)
# combine the selected molecules into one dataframe
qsarSmallData     <- rbind(data.raw[selectedActives,], data.raw[selectedInactives,])
```
Now, I load the `aid624202.sdf` file and compute the descriptor matrix. As the file is over 1 gb, it is imported using an iterator. The iterator goes through every molecule one by one and adds it to the `mols` list, if it's part of the selected Substance IDs. The code to compute the descriptor matrix is taken from [here](https://github.com/egonw/scientificProgramming/blob/master/assignment%202/descriptors.Rmd).
```{r eval = FALSE}
# select the SIDs from the selected subset of molecules
selectedSubstanceIDs = qsarSmallData[,"SID"]

# iload.molecules requires the full path
# creates an iterator to import the `aid624202.sdf` file
iter <- iload.molecules('/Users/felix/Google Drive/Uni/Systems Biology/Year 2/Scientific Programming/scientificProgramming2/aid624202.sdf', type='sdf')
# creates a txtProgressBar object to display the progress of importing the selected molecules
pb <- txtProgressBar(min = 0, max = num_total, style = 3)
hitsFound = 0   # initiate counter
mols = list()   # initiate list to hold the selected molecules
while(hasNext(iter)) {  # runs until 
  mol <- nextElem(iter) # set mol to the next molecule
  
  # get the SID of the current molecule
  sdfSID = get.property(mol, "PUBCHEM_SUBSTANCE_ID")
  # check whether the SID of the current molecule is part of the selected molecules
  if (sdfSID %in% selectedSubstanceIDs) {
    hitsFound = hitsFound + 1 # update the counter
    mols[[hitsFound]] = mol   # save the selected molecule in the list
    
    # stop when all hits are found instead of reading in all molecules
    if(hitsFound == num_total){
      break
    }
  }
  setTxtProgressBar(pb, hitsFound)
}
close(pb)

# get the names for the descriptor matrix
descNames <- unique(unlist(sapply(get.desc.categories()[2:4], get.desc.names)))

# compute the descriptor matrix
descs <- eval.desc(mols, descNames, verbose = TRUE)
class(descs)

# get the SID for the molecules
molSIDs <- unlist(lapply(mols, function(x) { get.property(x, "PUBCHEM_SUBSTANCE_ID")} ))

descriptor_file_name <- paste0('descriptors',num_total,'.csv')
# save the descriptor dataframe as a csv
write.csv(cbind(molSIDs, descs), file = descriptor_file_name, row.names=FALSE)
```
Then I combine the descriptor matrix with the score vector.
```{r}
# import the previously computed descriptor matrix
# import "descriptors.csv" instead to use the descriptors.csv 
# file with 1050 molecules
x <- read.csv("descriptors12200.csv",row=1) 
# row = 1 for descriptors12200
# row = 2 for others

# select the selected samples from the dataframe
data.raw <- data.raw[match(rownames(x),data.raw$SID),]

# add the acivity score to the descriptor dataframe (for plsr)
activity <- data.raw$Score
data     <- cbind(x,activity)
```
## Data cleaning
Columns that contain missing values and columns that are nearly constant are dropped from the dataframe.
```{r}
# Delete columns with missing values
descs <- data[, !apply(data, 2, function(x) any(is.na(x)) )]

# Delete columns with near zero variance using nearZeroVar from the caret package
badCols <- nearZeroVar(descs)
descs   <- descs[, -badCols]

# further filter out columns with low correlation
r2  <- which(cor(descs[1:(length(descs)-1)])^2 > .29, arr.ind=TRUE)
r2  <- r2[ r2[,1] > r2[,2] , ]
# temporary variable to hold the filtered descriptor dataframe
d   <- descs[, -unique(r2[,2])]
```
## Normalizing the data 
Then the data is normalised to stabilise its variance. The data transformation makes the data more normal distribution-like and improves the validity of measures of association such as the Pearson correlation between variables and for other data stabilization procedures.
```{r}
# use preProcess from the caret package to normalize the filtered data
data <- predict(preProcess(d[,1:dim(d)[2]],method = "BoxCox"),d[,2:dim(d)[2]])
```
## Subset the data
The data is subset into a testset/ trainset with a 20% / 80% split.
```{r}
# subset the data into train and test sets
ind<-sample(2,nrow(data),replace=TRUE,prob=c(0.8,0.2))
trainset_x<-data[ind==1,1:dim(data)[2]]
train_y<-trainset_x$activity
testset_x<-data[ind==2,1:dim(data)[2]]
test_y<- testset_x$activity
```

### Useful functions
Before starting the analysis, 2 functions are defined below.
```{r}
# Function to compute RMSE and R2 
r2se<- function (obs,pred){
  rmse<-(mean((obs - pred)^2))^0.5 
  ssr<-sum((obs - pred)^2)
  sst<-sum((obs - mean(obs))^2)
  R2<-1-(ssr/sst)
  output<-list(RMSE=rmse,RSquared=R2)
  return(output)
}

plotsar <-function(results){
  # plot the observed vs the predicted activity scores using ggplot
  return(ggplot(results,aes(x=observed,y=predicted,color=factor(set),shape=factor(set)))+geom_point()+scale_colour_manual(values=c("blue", "yellow"))+
          theme(axis.ticks.y=element_line(size=1),axis.text.y=element_text(size=16),axis.ticks.length=unit(0.25,"cm"), 
                panel.background = element_rect(fill = "black", colour = NA))+
          theme(axis.title=element_text(size=12,face="bold",colour = 'white'),plot.background = element_rect(colour = 'black', fill = 'black'))+
          theme( legend.title = element_text(size = 12, face = "bold", hjust = 0, colour = 'white'),
                legend.background=element_rect(color="black",fill="black"),legend.text=element_text(size=12,color="white",face="bold"),
                axis.title.x = element_text(size = 12, colour = 'white', vjust = 1) ,axis.title.y = element_text(size = 12, colour = 'white'))+
          ggtitle("Predicted vs test set QSAR results")+
          theme(plot.title=element_text(lineheight=.8,face="bold",color="white",size=14))+stat_smooth(span = 0.9))

}
```
## Use pls to model the data
```{r}
# fit a pls model to predict the activity
plsFit <- plsr(activity ~ ., data = trainset_x)

# Using the first ten components
pls.test <- data.frame(predict(plsFit, testset_x, ncomp = 1:10))
pls.train <- data.frame(predict(plsFit, trainset_x, ncomp = 1:10))
#----------------------------------------------------
# Summarizing results of test set
rlmValues <- data.frame(obs = test_y, pred = pls.test$activity.10.comps)

# print the RSME and R2
r2se(rlmValues$obs,rlmValues$pred)

# Create a dataframe for train and test set results for plotting
train.results<-data.frame(observed=train_y,predicted=pls.train$activity.5.comps,set="train")
test.results <- data.frame(observed=test_y,predicted=pls.test$activity.5.comps,set="test")
results<-rbind(train.results,test.results)

# plot the observed vs the predicted activity scores
plotsar(results)
```
## Tune the model using Cross Validation
Now, I am tuning the pls model with 10 fold cross validation and using up to 15 components.
```{r}
set.seed(42)      # to get consistent results during testing
tune_length <- 15 # set the max amount of components to use during tuning

# set up 10-fold cross validation
ctrl <- trainControl(method = "cv",number=10)
# tune the pls model with cross validation with different numbers of components
pls_tune <- train(subset(trainset_x,select=-c(activity)), train_y,
                 method = "pls",
                 tuneLength = tune_length,
                 trControl = ctrl,
                 preProc = "BoxCox")
```
The following code plots the RMSE over the number of components in the model.
```{r}
# plot results of pls_tune
plot(pls_tune)
```
Predict the values of the train set and the test set.
```{r}
# predict the activity for the train and test sets
pls_pred_train   <- predict(pls_tune,trainset_x)
pls_pred_test    <- predict(pls_tune,testset_x)

# set up dataframes to make calculating the error easier
pls_values_train <- data.frame(obs = train_y, pred = pls_pred_train)
pls_values_test  <- data.frame(obs = test_y, pred = pls_pred_test)

```
I calculate the RMSE and $R^2$ of the tuned pls results.
```{r}
r2se(pls_values_train$obs,pls_values_train$pred)
r2se(pls_values_test$obs,pls_values_test$pred)
```
Finally, I plot the regression plots of the train set and the test set. As you can see, I was not able to accurately predict the activity score using pls.
```{r}
# plot the observed vs the predicted activity scores of the trainset
y<-xyplot(trainset_x$activity ~ pls_pred_train, 
          type = c('g', 'p'),  xlab = "Predicted", ylab = "Observed",
          panel = function(x,y, ...){ 
            panel.xyplot(x,y,...)
            panel.lines(x, predict(pls_tune), col = 'black', lwd = 2) 
          } 
) 
# add the observed vs the predicted activity scores of the testset to the plot
y+as.layer(xyplot(test_y ~ pls_pred_test,pch=17,col="black"))
```
